# Sign Language Recognition Project - Complete Explanation

## ğŸ“‹ Project Overview
This is a **Real-time Sign Language Recognition System** using:
- **Frontend**: MediaPipe + WebSocket (live gesture detection)
- **Backend**: Flask + ONNX (inference server)
- **ML Model**: PyTorch MLP trained on landmark data

---

## ğŸ”„ Complete Workflow

### Phase 1: Data Collection
**File**: `save_landmarks.py`
```
1. Open webcam
2. Extract hand (21 points) + face (468 points) landmarks using MediaPipe
3. Create fixed-size vector (1530 values)
4. Save to JSONL with gesture label
5. Result: data_Hello.jsonl, data_Yes.jsonl, etc.
```

### Phase 2: Data Preprocessing
**File**: `preprocess/create_dataset.py`
```
1. Load all JSONL files from data_raw/
2. Normalize each vector using normalize_vector()
3. Combine into X.npy (features) and y.npy (labels)
4. Create classes.json mapping (gesture â†’ ID)
```

### Phase 3: Model Training
**File**: `model/train.py`
```
1. Load X.npy (1530-D features) and y.npy (labels)
2. Split 80/20 train/test
3. Train MLP: 1530 â†’ 512 â†’ 256 â†’ num_classes
4. Save as model.pth (PyTorch) and model.onnx (inference)
```

### Phase 4: Live Prediction
**Files**: `server.py` (backend) + `frontend/script.js` (frontend)
```
1. Frontend: Capture video â†’ MediaPipe extracts landmarks
2. Frontend: Send 1530-D vector to backend via WebSocket
3. Backend: Normalize vector â†’ ONNX model inference
4. Backend: Apply softmax â†’ Return prediction + confidence
5. Frontend: Display gesture name and score
```

---

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ save_landmarks.py           # Data collection (webcam recording)
â”œâ”€â”€ server.py                   # Backend Flask/SocketIO server
â”œâ”€â”€ classes.json                # Gesture label mapping
â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ preprocess/
â”‚   â”œâ”€â”€ create_dataset.py      # Load raw data â†’ Create X.npy, y.npy
â”‚   â”œâ”€â”€ normalize.py           # Normalization function (center + scale)
â”‚   â””â”€â”€ __pycache__/
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ train.py              # Train MLP model
â”‚   â”œâ”€â”€ model.pth             # PyTorch model weights
â”‚   â””â”€â”€ model.onnx            # ONNX model (for inference)
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html            # Main HTML page
â”‚   â”œâ”€â”€ script.js             # Frontend JavaScript (MediaPipe + WebSocket)
â”‚   â”œâ”€â”€ style.css             # CSS styling
â”‚   â””â”€â”€ mediapipe/            # MediaPipe resources
â”‚
â”œâ”€â”€ data_raw/                 # Raw JSONL files (recordings)
â”‚   â”œâ”€â”€ data_Hello.jsonl
â”‚   â”œâ”€â”€ data_Yes.jsonl
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ data_processed/           # Preprocessed numpy arrays
    â”œâ”€â”€ X.npy                 # Features (N, 1530)
    â””â”€â”€ y.npy                 # Labels (N,)
```

---

## ğŸ“Š Vector Dimensions

**Total Input Size**: 1530 values

```
Hand Landmarks (42 points):
  21 points/hand Ã— 2 hands Ã— 3 coords = 126 values
  
Face Landmarks (468 points):
  468 points Ã— 3 coords = 1404 values

Total: 126 + 1404 = 1530 values
```

---

## ğŸ”§ Key Functions

### normalize_vector() - `preprocess/normalize.py`
```python
1. Reshape (1530,) â†’ (510, 3)  [each landmark is x,y,z]
2. Find reference point (face center or hand center)
3. Center: Subtract reference from x,y coordinates
4. Scale: Divide by standard deviation
5. Return flattened (1530,) array
```

**Why?** Makes predictions consistent regardless of hand position/distance

### handle_landmark() - `server.py`
```python
1. Receive 1530-D vector from frontend
2. Normalize if needed
3. Run ONNX inference
4. Apply softmax to logits
5. Get highest probability class
6. Return gesture name + confidence score
```

### sendToServer() - `frontend/script.js`
```python
1. Check if hand detected (return if not)
2. Flatten hand landmarks (21 pts Ã— 3 = 63 values)
3. Flatten face landmarks (468 pts Ã— 3 = 1404 values)
4. Pad to exact 126 + 1404 = 1530 values
5. Send via SocketIO emit("landmark", {vector, normalized})
```

---

## ğŸš€ How to Run

### 1. Setup
```bash
# Create virtual environment
python -m venv env
.\env\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### 2. Collect Data (Optional)
```bash
python save_landmarks.py
# Then drag into data_raw/ folder
```

### 3. Train Model (Optional)
```bash
cd preprocess
python create_dataset.py
cd ../model
python train.py
```

### 4. Run Live Server
```bash
python server.py
# Open browser: http://localhost:5000
```

---

## ğŸ“ File Descriptions

| File | Purpose |
|------|---------|
| `server.py` | Backend: Flask server, ONNX inference, SocketIO events |
| `frontend/script.js` | Frontend: MediaPipe detection, WebSocket communication |
| `save_landmarks.py` | Data collection: Record gestures from webcam |
| `preprocess/create_dataset.py` | Load raw data, normalize, save as NumPy arrays |
| `preprocess/normalize.py` | Normalize function (center + scale landmarks) |
| `model/train.py` | Train MLP model and export to ONNX |

---

## ğŸ”Œ Communication Flow

```
Frontend (Browser)
    â†“
    | WebSocket (SocketIO)
    â†“
Backend Server (Flask)
    â†“
    | Normalize vector
    â†“
ONNX Model
    â†“
    | Softmax + argmax
    â†“
Prediction: {"label": "Hello", "score": 0.95}
    â†“
    | WebSocket (SocketIO)
    â†“
Frontend (Display on screen)
```

---

## ğŸ¯ Model Architecture

```
Input: 1530 values (hand + face landmarks)
    â†“
Linear(1530 â†’ 512) + ReLU
    â†“
Linear(512 â†’ 256) + ReLU
    â†“
Linear(256 â†’ num_classes)
    â†“
Output: Logits (raw scores)
    â†“
Softmax â†’ Probabilities
    â†“
Argmax â†’ Predicted gesture
```

---

## âœ… Dependencies

- **mediapipe**: Hand/face landmark detection
- **opencv-python**: Video processing
- **torch**, **torchvision**: Deep learning
- **onnx**, **onnxruntime**: Model export & inference
- **flask**, **flask-socketio**: Web server & real-time communication
- **numpy**: Data processing
- **eventlet**: Async support

---

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| "No hand detected" | Move hand into frame |
| Browser won't load | Check `http://localhost:5000` (not `0.0.0.0:5000`) |
| Low accuracy | Record more training data |
| Slow inference | Reduce model complexity or use GPU |

---

**Last Updated**: January 2026
